{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "input_path = 'path/to/input/images'\n",
    "output_path = 'path/to/output/predictions.csv'\n",
    "data_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] input_path output_path csv_path image_dir\n",
      "ipykernel_launcher.py: error: the following arguments are required: input_path, output_path, csv_path, image_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3450: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def load_data(csv_path, image_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['s'] = df['basename'].apply(lambda x: str(Path(image_dir) / x))\n",
    "    return df\n",
    "\n",
    "def create_data_generators(df, image_size=(128, 128), batch_size=32, validation_split=0.2):\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=validation_split)\n",
    "    \n",
    "    train_generator = datagen.flow_from_dataframe(\n",
    "        df,\n",
    "        x_col='filepath',\n",
    "        y_col='deepest_name',\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    \n",
    "    validation_generator = datagen.flow_from_dataframe(\n",
    "        df,\n",
    "        x_col='filepath',\n",
    "        y_col='deepest_name',\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    return train_generator, validation_generator\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def predict(model, inputs, image_size=(128, 128)):\n",
    "    images = []\n",
    "    for img_path in inputs:\n",
    "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=image_size)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = img / 255.0\n",
    "        images.append(img)\n",
    "    \n",
    "    images = np.vstack(images)\n",
    "    predictions = model.predict(images)\n",
    "    return predictions\n",
    "\n",
    "def main(input_path, output_path, csv_path, image_dir):\n",
    "    df = load_data(csv_path, image_dir)\n",
    "    train_generator, validation_generator = create_data_generators(df)\n",
    "    \n",
    "    input_shape = (128, 128, 3)\n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    \n",
    "    model = build_model(input_shape, num_classes)\n",
    "    \n",
    "    model.fit(train_generator, validation_data=validation_generator, epochs=10)\n",
    "    \n",
    "    inputs = list(Path(input_path).glob(\"*.jpg\"))\n",
    "    predictions = predict(model, inputs)\n",
    "    \n",
    "    values = {\n",
    "        \"image_uid\": [Path(input).stem for input in inputs],\n",
    "        \"predictions\": [train_generator.class_indices[pred] for pred in np.argmax(predictions, axis=1)]\n",
    "    }\n",
    "    \n",
    "    df_predictions = pd.DataFrame(values)\n",
    "    df_predictions.to_csv(output_path, index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\"input_path\", help=\"Path to input images\")\n",
    "    parser.add_argument(\"output_path\", help=\"Path to output csv\")\n",
    "    parser.add_argument(\"csv_path\", help=\"Path to CSV file with labels\")\n",
    "    parser.add_argument(\"image_dir\", help=\"Directory with images\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    main(args.input_path, args.output_path, args.csv_path, args.image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def load_data(csv_path, image_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['filepath'] = df['basename'].apply(lambda x: str(Path(image_dir) / x))\n",
    "    return df\n",
    "\n",
    "def create_data_generators(df, image_size=(128, 128), batch_size=32, validation_split=0.2):\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=validation_split)\n",
    "    \n",
    "    train_generator = datagen.flow_from_dataframe(\n",
    "        df,\n",
    "        x_col='filepath',\n",
    "        y_col='deepest_name',\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    \n",
    "    validation_generator = datagen.flow_from_dataframe(\n",
    "        df,\n",
    "        x_col='filepath',\n",
    "        y_col='deepest_name',\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    return train_generator, validation_generator\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def predict(model, inputs, image_size=(128, 128)):\n",
    "    images = []\n",
    "    for img_path in inputs:\n",
    "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=image_size)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = img / 255.0\n",
    "        images.append(img)\n",
    "    \n",
    "    images = np.vstack(images)\n",
    "    predictions = model.predict(images)\n",
    "    return predictions\n",
    "\n",
    "def main(input_path, output_path, csv_path, image_dir):\n",
    "    df = load_data(csv_path, image_dir)\n",
    "    \n",
    "    # Debugging line to ensure 'filepath' column exists\n",
    "    print(df.head())\n",
    "    \n",
    "    train_generator, validation_generator = create_data_generators(df)\n",
    "    \n",
    "    input_shape = (128, 128, 3)\n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    \n",
    "    model = build_model(input_shape, num_classes)\n",
    "    \n",
    "    model.fit(train_generator, validation_data=validation_generator, epochs=10)\n",
    "    \n",
    "    inputs = list(Path(input_path).glob(\"*.jpg\"))\n",
    "    predictions = predict(model, inputs)\n",
    "    \n",
    "    values = {\n",
    "        \"image_uid\": [Path(input).stem for input in inputs],\n",
    "        \"predictions\": [list(train_generator.class_indices.keys())[pred] for pred in np.argmax(predictions, axis=1)]\n",
    "    }\n",
    "    \n",
    "    df_predictions = pd.DataFrame(values)\n",
    "    df_predictions.to_csv(output_path, index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\"input_path\", help=\"Path to input images\")\n",
    "    parser.add_argument(\"output_path\", help=\"Path to output csv\")\n",
    "    parser.add_argument(\"csv_path\", help=\"Path to CSV file with labels\")\n",
    "    parser.add_argument(\"image_dir\", help=\"Directory with images\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    main(args.input_path, args.output_path, args.csv_path, args.image_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import pandas\n",
    "\n",
    "\n",
    "def predict(inputs: Path):    \n",
    "    inputs = list(inputs)\n",
    "    \n",
    "    # your algorithm goes here\n",
    "    \n",
    "    # delete and rewrite values with your predictions\n",
    "    values = {\n",
    "        \"image_uid\":[Path(input).stem for input in inputs],\n",
    "        \"level_0\": [\"Animalia\"] * len(inputs),\n",
    "        \"level_0_probability\": [1.0] * len(inputs)\n",
    "    }\n",
    "\n",
    "    return mapping_to_csv(values)\n",
    "\n",
    "def mapping_to_csv(values: dict[str: any]):\n",
    "    columns = [\"image_uid\",\"level_0\",\"level_0_probability\",\"level_1\",\"level_1_probability\",\"level_2\",\"level_2_probability\",\"level_3\",\"level_3_probability\",\"level_4\",\"level_4_probability\",\"level_5\",\"level_5_probability\"]\n",
    "\n",
    "    return pandas.DataFrame(columns=columns, data=values)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"input_path\", help=\"Path to input images\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"output_path\", help=\"Path to output csv\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    predict(Path(args.input_path).glob(\"*.jpg\")).to_csv(args.output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
